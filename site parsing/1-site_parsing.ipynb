{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77c41c8d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T12:20:37.612689Z",
     "iopub.status.busy": "2023-03-03T12:20:37.611904Z",
     "iopub.status.idle": "2023-03-03T12:20:37.624429Z",
     "shell.execute_reply": "2023-03-03T12:20:37.623239Z"
    },
    "papermill": {
     "duration": 0.020085,
     "end_time": "2023-03-03T12:20:37.627468",
     "exception": false,
     "start_time": "2023-03-03T12:20:37.607383",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c024d92c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T12:20:37.635987Z",
     "iopub.status.busy": "2023-03-03T12:20:37.635591Z",
     "iopub.status.idle": "2023-03-03T12:21:19.604697Z",
     "shell.execute_reply": "2023-03-03T12:21:19.603187Z"
    },
    "papermill": {
     "duration": 41.977631,
     "end_time": "2023-03-03T12:21:19.608108",
     "exception": false,
     "start_time": "2023-03-03T12:20:37.630477",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mCollecting faiss-cpu\r\n",
      "  Downloading faiss_cpu-1.7.3-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\r\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25hInstalling collected packages: faiss-cpu\r\n",
      "Successfully installed faiss-cpu-1.7.3\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0mRequirement already satisfied: implicit in /opt/conda/lib/python3.7/site-packages (0.4.4)\r\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.7/site-packages (from implicit) (1.21.6)\r\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from implicit) (4.64.0)\r\n",
      "Requirement already satisfied: scipy>=0.16 in /opt/conda/lib/python3.7/site-packages (from implicit) (1.7.3)\r\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\r\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import scipy\n",
    "import implicit\n",
    "import bisect\n",
    "import sklearn.metrics as m\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.calibration import calibration_curve, CalibratedClassifierCV\n",
    "!pip install feather-format >> none\n",
    "!pip install faiss-cpu --no-cache\n",
    "!pip install implicit\n",
    "import implicit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c1792edd",
   "metadata": {},
   "source": [
    "I get non-repetitive url_hosts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4995d7cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T12:21:19.656170Z",
     "iopub.status.busy": "2023-03-03T12:21:19.655262Z",
     "iopub.status.idle": "2023-03-03T12:22:05.388832Z",
     "shell.execute_reply": "2023-03-03T12:22:05.373391Z"
    },
    "papermill": {
     "duration": 45.755499,
     "end_time": "2023-03-03T12:22:05.403350",
     "exception": false,
     "start_time": "2023-03-03T12:21:19.647851",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0                                ad.adriver.ru\n",
       "1                                    apple.com\n",
       "2                       avatars.mds.yandex.net\n",
       "3                  googleads.g.doubleclick.net\n",
       "5                                  i.ytimg.com\n",
       "                           ...                \n",
       "322861818                        poliglot1.com\n",
       "322872650                     reservemaster.ru\n",
       "322882531                       uploadhive.com\n",
       "322883336            kuzma2012.livejournal.com\n",
       "322888794    stroitelnye-materialy-v-kurske.ru\n",
       "Name: url_host, Length: 199683, dtype: category\n",
       "Categories (199683, object): ['-1', '0--stranger-livejournal-com.turbopages.org', '0-1.ru', '0-34.ru', ..., 'юбилей-на-бис.рф', 'южныйокруг.рф', 'явернусь.рф', 'яркнига24.рф']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urls = pd.read_feather('/data preprocessing/data/dataset_full.feather')\n",
    "urls = urls['url_host'].drop_duplicates()\n",
    "urls"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "583a4089",
   "metadata": {},
   "source": [
    "Function for writing sites data to csv-file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e3bafb10",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T12:22:05.431607Z",
     "iopub.status.busy": "2023-03-03T12:22:05.431059Z",
     "iopub.status.idle": "2023-03-03T12:22:05.439039Z",
     "shell.execute_reply": "2023-03-03T12:22:05.437653Z"
    },
    "papermill": {
     "duration": 0.028937,
     "end_time": "2023-03-03T12:22:05.445130",
     "exception": false,
     "start_time": "2023-03-03T12:22:05.416193",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "def csv_writer(data):\n",
    "    with open('/data/sites.csv', 'a', encoding='utf-8') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(data)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ce9d9e9e",
   "metadata": {},
   "source": [
    "### Function for reading information from sites and writing it to file\n",
    "In this function I read html-page by url, parse description, title and keywords from html and write it to csv-file. There are 5 urls that crash script and I had to remove them:\n",
    "- sanstv.ru\n",
    "- t.lordfilms-filmy.online\n",
    "- z.lordfilms-film.online\n",
    "- n.lordfilm-smotret.one\n",
    "- z.lordfilmy-film.online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0a99f9c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-03T12:22:05.464188Z",
     "iopub.status.busy": "2023-03-03T12:22:05.463352Z",
     "iopub.status.idle": "2023-03-03T16:25:24.033391Z",
     "shell.execute_reply": "2023-03-03T16:25:24.031494Z"
    },
    "papermill": {
     "duration": 14598.588761,
     "end_time": "2023-03-03T16:25:24.044212",
     "exception": false,
     "start_time": "2023-03-03T12:22:05.455451",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n",
      "10000\n",
      "15000\n",
      "20000\n",
      "25000\n",
      "30000\n",
      "35000\n",
      "40000\n",
      "45000\n",
      "50000\n",
      "55000\n",
      "60000\n",
      "65000\n",
      "70000\n",
      "75000\n",
      "80000\n",
      "85000\n",
      "90000\n",
      "95000\n",
      "100000\n",
      "105000\n",
      "110000\n",
      "115000\n",
      "120000\n",
      "125000\n",
      "130000\n",
      "135000\n",
      "140000\n",
      "145000\n",
      "150000\n",
      "155000\n",
      "160000\n",
      "165000\n",
      "170000\n",
      "175000\n",
      "180000\n",
      "185000\n",
      "190000\n",
      "195000\n",
      "199683\n",
      "CPU times: user 32.8 s, sys: 59.7 s, total: 1min 32s\n",
      "Wall time: 4h 3min 18s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from multiprocessing import Pool\n",
    "from urllib3.exceptions import LocationParseError\n",
    "\n",
    "def parse_url(url):\n",
    "    url_data = np.empty([5], dtype=object)\n",
    "    url_data[0] = url[0]\n",
    "    url_data[1] = url[1]\n",
    "    try:\n",
    "        if (url[1] == 'sanstv.ru'):\n",
    "            csv_writer(url_data)       \n",
    "            del(url_data)\n",
    "            return\n",
    "        if (url[1] == 't.lordfilms-filmy.online'):\n",
    "            csv_writer(url_data)       \n",
    "            del(url_data)\n",
    "            return\n",
    "        if (url[1] == 'z.lordfilms-film.online'):\n",
    "            csv_writer(url_data)       \n",
    "            del(url_data)\n",
    "            return\n",
    "        if (url[1] == 'n.lordfilm-smotret.one'):\n",
    "            csv_writer(url_data)       \n",
    "            del(url_data)\n",
    "            return\n",
    "        if (url[1] == 'z.lordfilmy-film.online'):\n",
    "            csv_writer(url_data)       \n",
    "            del(url_data)\n",
    "            return\n",
    "        html = requests.get(f\"http://{url[1]}\", timeout=6, headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/110.0.0.0 Safari/537.36', \n",
    "                                                           'Accept': 'image/avif,image/webp,image/apng,image/svg+xml,image/*,*/*;q=0.8',\n",
    "                                                           'Accept-Encoding': 'gzip, deflate, br',\n",
    "                                                           'Accept-Language':  'ru-RU,ru;q=0.9,en-US;q=0.8,en;q=0.7',\n",
    "                                                           'Cookie': 'vid=1b448c0013086e08644a; mkmgsgp=RQXPUS'})\n",
    "        # Due to problems with encoding in sites I forced decode text as utf-8 and I for bad decoded sites I use cp1251\n",
    "        html.encoding = 'utf-8'\n",
    "        soup = BeautifulSoup(html.text, 'lxml')\n",
    "        descr = soup.find('meta', attrs={\"name\": \"Description\", 'content' : True})\n",
    "        if descr is not None:\n",
    "            url_data[3] = descr['content']\n",
    "        descr = soup.find('meta', attrs={\"name\": \"description\", 'content' : True})\n",
    "        if descr is not None:\n",
    "            url_data[3] = descr['content']\n",
    "        del(descr)\n",
    "        title = soup.find('title')\n",
    "        if title is not None:\n",
    "            url_data[2] = title.text\n",
    "        del(title)\n",
    "        keywords = soup.find('meta', attrs={\"name\": \"keywords\", 'content' : True})\n",
    "        if keywords is not None:\n",
    "            url_data[4] = keywords['content']\n",
    "        keywords = soup.find('meta', attrs={\"name\": \"Keywords\", 'content' : True})\n",
    "        if keywords is not None:\n",
    "            url_data[4] = keywords['content']\n",
    "        del(keywords)\n",
    "        csv_writer(url_data)\n",
    "        del(html)\n",
    "        del(soup)     \n",
    "        del(url_data)\n",
    "    except AttributeError: \n",
    "        csv_writer(url_data)       \n",
    "        del(url_data)\n",
    "    except requests.exceptions.RequestException: \n",
    "        csv_writer(url_data)       \n",
    "        del(url_data)\n",
    "    except LocationParseError: \n",
    "        csv_writer(url_data)       \n",
    "        del(url_data)  \n",
    "\n",
    "\n",
    "i = 0\n",
    "j = 5000\n",
    "while (j < 199684):\n",
    "    # I use 40 thread for parsing\n",
    "    with Pool(40) as p: \n",
    "        p.map(parse_url, enumerate(urls[urls.index[range(i, j)]]))\n",
    "    i = j\n",
    "    print(i)\n",
    "    if (j != 195000):\n",
    "        j += 5000\n",
    "    else:\n",
    "        j = 199683"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 14697.735338,
   "end_time": "2023-03-03T16:25:25.207521",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-03T12:20:27.472183",
   "version": "2.3.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
