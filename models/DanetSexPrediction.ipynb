{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SorokinMaksimArtemovich/MTS-ML-CUP/blob/main/models/DanetSexPrediction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "import warnings\n",
        "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
        "warnings.filterwarnings('ignore')"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.031093,
          "end_time": "2023-02-05T11:37:04.379248",
          "exception": false,
          "start_time": "2023-02-05T11:37:04.348155",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-03-22T04:06:16.321139Z",
          "iopub.execute_input": "2023-03-22T04:06:16.321556Z",
          "iopub.status.idle": "2023-03-22T04:06:16.351485Z",
          "shell.execute_reply.started": "2023-03-22T04:06:16.321471Z",
          "shell.execute_reply": "2023-03-22T04:06:16.350513Z"
        },
        "trusted": true,
        "id": "g3JfUnHTgDiC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import pyarrow as pa\n",
        "import pyarrow.parquet as pq\n",
        "import scipy\n",
        "import gc\n",
        "!pip install implicit\n",
        "import implicit\n",
        "import bisect\n",
        "import pickle\n",
        "import sklearn.metrics as m\n",
        "!pip install catboost\n",
        "from catboost import CatBoostClassifier, CatBoostRegressor, Pool\n",
        "from sklearn.calibration import calibration_curve, CalibratedClassifierCV"
      ],
      "metadata": {
        "papermill": {
          "duration": 1.51524,
          "end_time": "2023-02-05T11:37:05.905128",
          "exception": false,
          "start_time": "2023-02-05T11:37:04.389888",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-03-22T04:06:16.380747Z",
          "iopub.execute_input": "2023-03-22T04:06:16.381082Z",
          "iopub.status.idle": "2023-03-22T04:06:17.718846Z",
          "shell.execute_reply.started": "2023-03-22T04:06:16.381052Z",
          "shell.execute_reply": "2023-03-22T04:06:17.717855Z"
        },
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ld9gREnHgDiO",
        "outputId": "14f88d00-4ae1-469d-df28-8873bdf0bfd4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting implicit\n",
            "  Downloading implicit-0.6.2-cp39-cp39-manylinux2014_x86_64.whl (18.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.6/18.6 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.9/dist-packages (from implicit) (1.22.4)\n",
            "Requirement already satisfied: scipy>=0.16 in /usr/local/lib/python3.9/dist-packages (from implicit) (1.10.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from implicit) (4.65.0)\n",
            "Installing collected packages: implicit\n",
            "Successfully installed implicit-0.6.2\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting catboost\n",
            "  Downloading catboost-1.1.1-cp39-none-manylinux1_x86_64.whl (76.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.6/76.6 MB\u001b[0m \u001b[31m13.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.9/dist-packages (from catboost) (1.10.1)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.9/dist-packages (from catboost) (0.20.1)\n",
            "Requirement already satisfied: numpy>=1.16.0 in /usr/local/lib/python3.9/dist-packages (from catboost) (1.22.4)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.9/dist-packages (from catboost) (5.13.1)\n",
            "Requirement already satisfied: pandas>=0.24.0 in /usr/local/lib/python3.9/dist-packages (from catboost) (1.4.4)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.9/dist-packages (from catboost) (3.7.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.9/dist-packages (from catboost) (1.16.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->catboost) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.9/dist-packages (from pandas>=0.24.0->catboost) (2022.7.1)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (8.4.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (1.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (1.4.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (23.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (0.11.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (3.0.9)\n",
            "Requirement already satisfied: importlib-resources>=3.2.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (5.12.0)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.9/dist-packages (from matplotlib->catboost) (4.39.3)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.9/dist-packages (from plotly->catboost) (8.2.2)\n",
            "Requirement already satisfied: zipp>=3.1.0 in /usr/local/lib/python3.9/dist-packages (from importlib-resources>=3.2.0->matplotlib->catboost) (3.15.0)\n",
            "Installing collected packages: catboost\n",
            "Successfully installed catboost-1.1.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import plotly.express as px\n",
        "\n",
        "%matplotlib inline\n",
        "sns.set_style('darkgrid')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-22T04:06:17.720954Z",
          "iopub.execute_input": "2023-03-22T04:06:17.721710Z",
          "iopub.status.idle": "2023-03-22T04:06:19.874502Z",
          "shell.execute_reply.started": "2023-03-22T04:06:17.721648Z",
          "shell.execute_reply": "2023-03-22T04:06:19.873413Z"
        },
        "trusted": true,
        "id": "dsFOrdYOgDiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#!pip install feather-format >> none\n",
        "#!pip install faiss-cpu --no-cache"
      ],
      "metadata": {
        "papermill": {
          "duration": 13.171112,
          "end_time": "2023-02-05T11:37:19.119255",
          "exception": false,
          "start_time": "2023-02-05T11:37:05.948143",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-03-22T04:06:19.876183Z",
          "iopub.execute_input": "2023-03-22T04:06:19.876902Z",
          "iopub.status.idle": "2023-03-22T04:06:19.881372Z",
          "shell.execute_reply.started": "2023-03-22T04:06:19.876865Z",
          "shell.execute_reply": "2023-03-22T04:06:19.880346Z"
        },
        "trusted": true,
        "id": "n_46yNNlgDiP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def age_bucket(x):\n",
        "    return bisect.bisect_left([18,25,35,45,55,65], x)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-22T04:06:19.884177Z",
          "iopub.execute_input": "2023-03-22T04:06:19.885204Z",
          "iopub.status.idle": "2023-03-22T04:06:19.893361Z",
          "shell.execute_reply.started": "2023-03-22T04:06:19.885165Z",
          "shell.execute_reply": "2023-03-22T04:06:19.892409Z"
        },
        "trusted": true,
        "id": "EQoPQLXsgDiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### save and load functions"
      ],
      "metadata": {
        "id": "prGQ8QDjiEqN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def save(obj, path, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Saving object to {}\".format(path))\n",
        "\n",
        "    with open(path, \"wb\") as obj_file:\n",
        "        pickle.dump( obj, obj_file, protocol=pickle.HIGHEST_PROTOCOL )\n",
        "\n",
        "    if verbose:\n",
        "        print(\"Object saved to {}\".format(path))\n",
        "    pass"
      ],
      "metadata": {
        "trusted": true,
        "id": "8_MWN-SZgDic"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load(path, verbose=True):\n",
        "    if verbose:\n",
        "        print(\"Loading object from {}\".format(path))\n",
        "    with open(path, \"rb\") as obj_file:\n",
        "        obj = pickle.load(obj_file)\n",
        "    if verbose:\n",
        "        print(\"Object loaded from {}\".format(path))\n",
        "    return obj"
      ],
      "metadata": {
        "id": "_2JP5VCwIsKJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### importing data and reading train targets and test id"
      ],
      "metadata": {
        "id": "rBztoPiUhh3_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q kaggle"
      ],
      "metadata": {
        "id": "LJQ-6yAG8Ubg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "files.upload()"
      ],
      "metadata": {
        "id": "pUP8XudK8U_j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        },
        "outputId": "f12ef8d8-7e5e-486c-b9d1-615388983b9a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-4dd291cc-ce6e-4faf-ab6f-0b11910af142\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-4dd291cc-ce6e-4faf-ab6f-0b11910af142\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'kaggle.json': b'{\"username\":\"sorokinmaksim\",\"key\":\"9537fa921e351600c84c9f61d1f01441\"}'}"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!ls -lha kaggle.json"
      ],
      "metadata": {
        "id": "57_Sanpz8VKt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c5b595f-fad7-403c-a7c8-3723581435a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-rw-r--r-- 1 root root 69 Apr  7 11:23 kaggle.json\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "ulN0yhmA8x_M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!kaggle datasets download -d sorokinmaksim/mts-ml-cup-data-for-training-model"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btjzUq850inT",
        "outputId": "da8d77cb-46f2-4151-b073-957da6b172a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading mts-ml-cup-data-for-training-model.zip to /content\n",
            "100% 4.43G/4.43G [04:17<00:00, 18.9MB/s]\n",
            "100% 4.43G/4.43G [04:17<00:00, 18.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/mts-ml-cup-data-for-training-model.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l5vZed92zrmC",
        "outputId": "7e544f38-c437-442a-8390-8d5c395d6f1e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/mts-ml-cup-data-for-training-model.zip\n",
            "  inflating: df_age.csv              \n",
            "  inflating: df_age_1.csv            \n",
            "  inflating: df_danet.csv            \n",
            "  inflating: df_sex.csv              \n",
            "  inflating: df_sex_1.csv            \n",
            "  inflating: id_to_submit.csv        \n",
            "  inflating: none                    \n",
            "  inflating: targets.csv             \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "id_to_submit = pd.read_csv('id_to_submit.csv')"
      ],
      "metadata": {
        "papermill": {
          "duration": 0.101198,
          "end_time": "2023-02-05T11:37:19.26373",
          "exception": false,
          "start_time": "2023-02-05T11:37:19.162532",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-03-22T04:06:19.896057Z",
          "iopub.execute_input": "2023-03-22T04:06:19.897545Z",
          "iopub.status.idle": "2023-03-22T04:06:19.951142Z",
          "shell.execute_reply.started": "2023-03-22T04:06:19.897506Z",
          "shell.execute_reply": "2023-03-22T04:06:19.950014Z"
        },
        "trusted": true,
        "id": "ZuAc9o9CgDiQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "targets = pd.read_csv('targets.csv')"
      ],
      "metadata": {
        "papermill": {
          "duration": 40.532659,
          "end_time": "2023-02-05T11:37:59.887636",
          "exception": false,
          "start_time": "2023-02-05T11:37:19.354977",
          "status": "completed"
        },
        "tags": [],
        "execution": {
          "iopub.status.busy": "2023-03-22T04:06:19.953736Z",
          "iopub.execute_input": "2023-03-22T04:06:19.956937Z",
          "iopub.status.idle": "2023-03-22T04:06:20.093367Z",
          "shell.execute_reply.started": "2023-03-22T04:06:19.956895Z",
          "shell.execute_reply": "2023-03-22T04:06:20.092253Z"
        },
        "trusted": true,
        "id": "YFecotj_gDiR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### standart scaling and making test and train data function"
      ],
      "metadata": {
        "id": "JCka_hFXhOi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "def make_train_data(data):\n",
        "  scaler = StandardScaler()\n",
        "  data_id = data['user_id']\n",
        "  df = pd.DataFrame(scaler.fit_transform(data.drop(['user_id'], axis=1)), columns=data.drop(['user_id'], axis=1).columns)\n",
        "  df['user_id'] = data_id\n",
        "  df_sex_test = id_to_submit.merge(df, how = 'left', on = ['user_id'])\n",
        "  print(df_sex_test.shape)\n",
        "  df_sex = targets.merge(df, how = 'inner', on = ['user_id'])\n",
        "  df_sex = df_sex[df_sex['is_male'] != 'NA']\n",
        "  df_sex = df_sex.dropna()\n",
        "  df_sex['is_male'] = df_sex['is_male'].map(int)\n",
        "  print(df_sex['is_male'].value_counts())\n",
        "  del(df)\n",
        "  del(data)\n",
        "  gc.collect()\n",
        "  return df_sex, df_sex_test"
      ],
      "metadata": {
        "id": "2duDYJK1U1vL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# DaNet"
      ],
      "metadata": {
        "id": "eHlZHgC2gPnC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch import nn\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "\n",
        "import torch\n",
        "\n",
        "\"\"\"\n",
        "Other possible implementations:\n",
        "https://github.com/KrisKorrel/sparsemax-pytorch/blob/master/sparsemax.py\n",
        "https://github.com/msobroza/SparsemaxPytorch/blob/master/mnist/sparsemax.py\n",
        "https://github.com/vene/sparse-structured-attention/blob/master/pytorch/torchsparseattn/sparsemax.py\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# credits to Yandex https://github.com/Qwicen/node/blob/master/lib/nn_utils.py\n",
        "def _make_ix_like(input, dim=0):\n",
        "    d = input.size(dim)\n",
        "    rho = torch.arange(1, d + 1, device=input.device, dtype=input.dtype)\n",
        "    view = [1] * input.dim()\n",
        "    view[0] = -1\n",
        "    return rho.view(view).transpose(0, dim)\n",
        "\n",
        "\n",
        "class SparsemaxFunction(Function):\n",
        "    \"\"\"\n",
        "    An implementation of sparsemax (Martins & Astudillo, 2016). See\n",
        "    :cite:`DBLP:journals/corr/MartinsA16` for detailed description.\n",
        "    By Ben Peters and Vlad Niculae\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, dim=-1):\n",
        "        \"\"\"sparsemax: normalizing sparse transform (a la softmax)\n",
        "        Parameters\n",
        "        ----------\n",
        "        ctx : torch.autograd.function._ContextMethodMixin\n",
        "        input : torch.Tensor\n",
        "            any shape\n",
        "        dim : int\n",
        "            dimension along which to apply sparsemax\n",
        "        Returns\n",
        "        -------\n",
        "        output : torch.Tensor\n",
        "            same shape as input\n",
        "        \"\"\"\n",
        "        ctx.dim = dim\n",
        "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
        "        input -= max_val  # same numerical stability trick as for softmax\n",
        "        tau, supp_size = SparsemaxFunction._threshold_and_support(input, dim=dim)\n",
        "        output = torch.clamp(input - tau, min=0)\n",
        "        ctx.save_for_backward(supp_size, output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        supp_size, output = ctx.saved_tensors\n",
        "        dim = ctx.dim\n",
        "        grad_input = grad_output.clone()\n",
        "        grad_input[output == 0] = 0\n",
        "\n",
        "        v_hat = grad_input.sum(dim=dim) / supp_size.to(output.dtype).squeeze()\n",
        "        v_hat = v_hat.unsqueeze(dim)\n",
        "        grad_input = torch.where(output != 0, grad_input - v_hat, grad_input)\n",
        "        return grad_input, None\n",
        "\n",
        "    @staticmethod\n",
        "    def _threshold_and_support(input, dim=-1):\n",
        "        \"\"\"Sparsemax building block: compute the threshold\n",
        "        Parameters\n",
        "        ----------\n",
        "        input: torch.Tensor\n",
        "            any dimension\n",
        "        dim : int\n",
        "            dimension along which to apply the sparsemax\n",
        "        Returns\n",
        "        -------\n",
        "        tau : torch.Tensor\n",
        "            the threshold value\n",
        "        support_size : torch.Tensor\n",
        "        \"\"\"\n",
        "\n",
        "        input_srt, _ = torch.sort(input, descending=True, dim=dim)\n",
        "        input_cumsum = input_srt.cumsum(dim) - 1\n",
        "        rhos = _make_ix_like(input, dim)\n",
        "        support = rhos * input_srt > input_cumsum\n",
        "\n",
        "        support_size = support.sum(dim=dim).unsqueeze(dim)\n",
        "        tau = input_cumsum.gather(dim, support_size - 1)\n",
        "        tau /= support_size.to(input.dtype)\n",
        "        return tau, support_size\n",
        "\n",
        "\n",
        "sparsemax = SparsemaxFunction.apply\n",
        "\n",
        "\n",
        "class Sparsemax(nn.Module):\n",
        "\n",
        "    def __init__(self, dim=-1):\n",
        "        self.dim = dim\n",
        "        super(Sparsemax, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return sparsemax(input, self.dim)\n",
        "\n",
        "\n",
        "class Entmax15Function(Function):\n",
        "    \"\"\"\n",
        "    An implementation of exact Entmax with alpha=1.5 (B. Peters, V. Niculae, A. Martins). See\n",
        "    :cite:`https://arxiv.org/abs/1905.05702 for detailed description.\n",
        "    Source: https://github.com/deep-spin/entmax\n",
        "    \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input, dim=-1):\n",
        "        ctx.dim = dim\n",
        "\n",
        "        max_val, _ = input.max(dim=dim, keepdim=True)\n",
        "        input = input - max_val  # same numerical stability trick as for softmax\n",
        "        input = input / 2  # divide by 2 to solve actual Entmax\n",
        "\n",
        "        tau_star, _ = Entmax15Function._threshold_and_support(input, dim)\n",
        "        output = torch.clamp(input - tau_star, min=0) ** 2\n",
        "        ctx.save_for_backward(output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        Y, = ctx.saved_tensors\n",
        "        gppr = Y.sqrt()  # = 1 / g'' (Y)\n",
        "        dX = grad_output * gppr\n",
        "        q = dX.sum(ctx.dim) / gppr.sum(ctx.dim)\n",
        "        q = q.unsqueeze(ctx.dim)\n",
        "        dX -= q * gppr\n",
        "        return dX, None\n",
        "\n",
        "    @staticmethod\n",
        "    def _threshold_and_support(input, dim=-1):\n",
        "        Xsrt, _ = torch.sort(input, descending=True, dim=dim)\n",
        "\n",
        "        rho = _make_ix_like(input, dim)\n",
        "        mean = Xsrt.cumsum(dim) / rho\n",
        "        mean_sq = (Xsrt ** 2).cumsum(dim) / rho\n",
        "        ss = rho * (mean_sq - mean ** 2)\n",
        "        delta = (1 - ss) / rho\n",
        "\n",
        "        # NOTE this is not exactly the same as in reference algo\n",
        "        # Fortunately it seems the clamped values never wrongly\n",
        "        # get selected by tau <= sorted_z. Prove this!\n",
        "        delta_nz = torch.clamp(delta, 0)\n",
        "        tau = mean - torch.sqrt(delta_nz)\n",
        "\n",
        "        support_size = (tau <= Xsrt).sum(dim).unsqueeze(dim)\n",
        "        tau_star = tau.gather(dim, support_size - 1)\n",
        "        return tau_star, support_size\n",
        "\n",
        "\n",
        "class Entmoid15(Function):\n",
        "    \"\"\" A highly optimized equivalent of lambda x: Entmax15([x, 0]) \"\"\"\n",
        "\n",
        "    @staticmethod\n",
        "    def forward(ctx, input):\n",
        "        output = Entmoid15._forward(input)\n",
        "        ctx.save_for_backward(output)\n",
        "        return output\n",
        "\n",
        "    @staticmethod\n",
        "    def _forward(input):\n",
        "        input, is_pos = abs(input), input >= 0\n",
        "        tau = (input + torch.sqrt(F.relu(8 - input ** 2))) / 2\n",
        "        tau.masked_fill_(tau <= input, 2.0)\n",
        "        y_neg = 0.25 * F.relu(tau - input, inplace=True) ** 2\n",
        "        return torch.where(is_pos, 1 - y_neg, y_neg)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        return Entmoid15._backward(ctx.saved_tensors[0], grad_output)\n",
        "\n",
        "    @staticmethod\n",
        "    def _backward(output, grad_output):\n",
        "        gppr0, gppr1 = output.sqrt(), (1 - output).sqrt()\n",
        "        grad_input = grad_output * gppr0\n",
        "        q = grad_input / (gppr0 + gppr1)\n",
        "        grad_input -= q * gppr0\n",
        "        return grad_input\n",
        "\n",
        "\n",
        "entmax15 = Entmax15Function.apply\n",
        "entmoid15 = Entmoid15.apply\n",
        "\n",
        "\n",
        "class Entmax15(nn.Module):\n",
        "\n",
        "    def __init__(self, dim=-1):\n",
        "        self.dim = dim\n",
        "        super(Entmax15, self).__init__()\n",
        "\n",
        "    def forward(self, input):\n",
        "        return entmax15(input, self.dim)"
      ],
      "metadata": {
        "trusted": true,
        "id": "QCkYYw3XgDiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch.nn.functional as F\n",
        "from scipy.special import softmax\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.nn.functional import cross_entropy, mse_loss\n",
        "\n",
        "def initialize_glu(module, input_dim, output_dim):\n",
        "    gain_value = np.sqrt((input_dim + output_dim) / np.sqrt(input_dim))\n",
        "    torch.nn.init.xavier_normal_(module.weight, gain=gain_value)\n",
        "    return\n",
        "\n",
        "class GBN(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Ghost Batch Normalization\n",
        "    https://arxiv.org/abs/1705.08741\n",
        "    \"\"\"\n",
        "    def __init__(self, input_dim, virtual_batch_size=512):\n",
        "        super(GBN, self).__init__()\n",
        "        self.input_dim = input_dim\n",
        "        self.virtual_batch_size = virtual_batch_size\n",
        "        self.bn = nn.BatchNorm1d(self.input_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        if self.training == True:\n",
        "            chunks = x.chunk(int(np.ceil(x.shape[0] / self.virtual_batch_size)), 0)\n",
        "            res = [self.bn(x_) for x_ in chunks]\n",
        "            return torch.cat(res, dim=0)\n",
        "        else:\n",
        "            return self.bn(x)\n",
        "\n",
        "class LearnableLocality(nn.Module):\n",
        "\n",
        "    def __init__(self, input_dim, k):\n",
        "        super(LearnableLocality, self).__init__()\n",
        "        self.register_parameter('weight', nn.Parameter(torch.rand(k, input_dim)))\n",
        "        self.smax = Entmax15(dim=-1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mask = self.smax(self.weight)\n",
        "        masked_x = torch.einsum('nd,bd->bnd', mask, x)  # [B, k, D]\n",
        "        return masked_x\n",
        "\n",
        "class AbstractLayer(nn.Module):\n",
        "    def __init__(self, base_input_dim, base_output_dim, k, virtual_batch_size, bias=True):\n",
        "        super(AbstractLayer, self).__init__()\n",
        "        self.masker = LearnableLocality(input_dim=base_input_dim, k=k)\n",
        "        self.fc = nn.Conv1d(base_input_dim * k, 2 * k * base_output_dim, kernel_size=1, groups=k, bias=bias)\n",
        "        initialize_glu(self.fc, input_dim=base_input_dim * k, output_dim=2 * k * base_output_dim)\n",
        "        self.bn = GBN(2 * base_output_dim * k, virtual_batch_size)\n",
        "        self.k = k\n",
        "        self.base_output_dim = base_output_dim\n",
        "\n",
        "    def forward(self, x):\n",
        "        b = x.size(0)\n",
        "        x = self.masker(x)  # [B, D] -> [B, k, D]\n",
        "        x = self.fc(x.view(b, -1, 1))  # [B, k, D] -> [B, k * D, 1] -> [B, k * (2 * D'), 1]\n",
        "        x = self.bn(x)\n",
        "        chunks = x.chunk(self.k, 1)  # k * [B, 2 * D', 1]\n",
        "        x = sum([F.relu(torch.sigmoid(x_[:, :self.base_output_dim, :]) * x_[:, self.base_output_dim:, :]) for x_ in chunks])  # k * [B, D', 1] -> [B, D', 1]\n",
        "        return x.squeeze(-1)\n",
        "\n",
        "\n",
        "class BasicBlock(nn.Module):\n",
        "    def __init__(self, input_dim, base_outdim, k, virtual_batch_size, fix_input_dim, drop_rate):\n",
        "        super(BasicBlock, self).__init__()\n",
        "        self.conv1 = AbstractLayer(input_dim, base_outdim // 2, k, virtual_batch_size)\n",
        "        self.conv2 = AbstractLayer(base_outdim // 2, base_outdim, k, virtual_batch_size)\n",
        "\n",
        "        self.downsample = nn.Sequential(\n",
        "            nn.Dropout(drop_rate),\n",
        "            AbstractLayer(fix_input_dim, base_outdim, k, virtual_batch_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x, pre_out=None):\n",
        "        if pre_out == None:\n",
        "            pre_out = x\n",
        "        out = self.conv1(pre_out)\n",
        "        out = self.conv2(out)\n",
        "        identity = self.downsample(x)\n",
        "        out += identity\n",
        "        return F.leaky_relu(out, 0.01)\n",
        "\n",
        "\n",
        "class DANet(nn.Module):\n",
        "    def __init__(self, input_dim, num_classes, layer_num=20, base_outdim=64, k=5, virtual_batch_size=256, drop_rate=0.1):\n",
        "        super(DANet, self).__init__()\n",
        "        params = {'base_outdim': base_outdim, 'k': k, 'virtual_batch_size': virtual_batch_size,\n",
        "                  'fix_input_dim': input_dim, 'drop_rate': drop_rate}\n",
        "        self.init_layer = BasicBlock(input_dim, **params)\n",
        "        self.lay_num = layer_num\n",
        "        self.layer = nn.ModuleList()\n",
        "        for i in range((layer_num // 2) - 1):\n",
        "            self.layer.append(BasicBlock(base_outdim, **params))\n",
        "        self.drop = nn.Dropout(0.1)\n",
        "\n",
        "        self.fc = nn.Sequential(nn.Linear(base_outdim, 256),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Linear(256, 512),\n",
        "                                nn.ReLU(inplace=True),\n",
        "                                nn.Linear(512, num_classes))\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.init_layer(x)\n",
        "        for i in range(len(self.layer)):\n",
        "            out = self.layer[i](x, out)\n",
        "        out = self.drop(out)\n",
        "        out = self.fc(out)\n",
        "        return out"
      ],
      "metadata": {
        "trusted": true,
        "id": "6cTQZ7W8gDiZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from copy import deepcopy\n",
        "from datetime import datetime\n",
        "\n",
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torch.optim import SGD, Adam, AdamW\n",
        "!pip install qhoptim\n",
        "from qhoptim.pyt import QHAdam\n",
        "\n",
        "\n",
        "class CustomDataset(Dataset):\n",
        "    def __init__(self, x, y, class_count):\n",
        "        \n",
        "        self.x = x.astype( np.float32 )\n",
        "        self.y = y.astype( np.float32 )\n",
        "        self.ds_len = len(y)\n",
        "        self.class_count = class_count\n",
        "        \n",
        "        y_transformed = np.zeros((self.ds_len, self.class_count), dtype=np.float32)\n",
        "        for i in range(len(y)):\n",
        "            y_transformed[i][y[i]] = 1.0\n",
        "        self.y = y_transformed\n",
        "    \n",
        "        pass\n",
        "    \n",
        "    def __getitem__(self, id):\n",
        "        \n",
        "        x = self.x[id]\n",
        "        y = self.y[id]\n",
        "        \n",
        "        return x, y\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.ds_len\n",
        "    \n",
        "class PredictDataset(Dataset):\n",
        "    def __init__(self, x):\n",
        "        self.x = x.astype( np.float32 )\n",
        "        self.ds_len = len(x)\n",
        "        pass\n",
        "    \n",
        "    def __getitem__(self, id):\n",
        "        x = self.x[id]\n",
        "        return x\n",
        "    \n",
        "    def __len__(self):\n",
        "        return self.ds_len \n",
        "        \n",
        "        \n",
        "\n",
        "class DANetClassifier():\n",
        "    def __init__(self, input_dim, num_classes, \n",
        "                 #layer_num=48, base_outdim=96, k=8,\n",
        "                 layer_num=32, base_outdim=64, k=5,\n",
        "                 virtual_batch_size=256, drop_rate=0.1,\n",
        "                 device=\"cuda\"):\n",
        "        \n",
        "        self.device = device\n",
        "        \n",
        "        self.danet = DANet(input_dim = input_dim, \n",
        "                           num_classes = num_classes, \n",
        "                           layer_num = layer_num, \n",
        "                           base_outdim = base_outdim, \n",
        "                           k = k, \n",
        "                           virtual_batch_size = virtual_batch_size, \n",
        "                           drop_rate = drop_rate)\n",
        "        self.model = torch.nn.Sequential( self.danet, nn.LogSoftmax(dim=1) )\n",
        "        self.model = self.model.to( self.device )\n",
        "        \n",
        "        self.class_names = None\n",
        "        \n",
        "        pass\n",
        "    \n",
        "    def predict_proba(self, x, batch_size=1024):\n",
        "        \n",
        "        self.model.eval()\n",
        "        \n",
        "        predict_dataset = PredictDataset( x )\n",
        "        predict_dataloader = DataLoader( predict_dataset, batch_size=batch_size, shuffle=False )\n",
        "        \n",
        "        probas = []\n",
        "        for x_batch in predict_dataloader:\n",
        "            x_batch = x_batch.to( self.device )\n",
        "            y_pred = self.model( x_batch )\n",
        "            y_pred = nn.Softmax(dim=1)(y_pred)\n",
        "            y_pred = y_pred.cpu().detach().numpy()\n",
        "            probas.append( y_pred )\n",
        "        probas = np.vstack( probas )\n",
        "        \n",
        "        return probas\n",
        "    \n",
        "    def predict(self, x, batch_size=1024):\n",
        "        \n",
        "        probas = self.predict_proba(x, batch_size)\n",
        "        \n",
        "        y_pred = []\n",
        "        for i in range(len(probas)):\n",
        "            current_proba = probas[i]\n",
        "            y_i = np.argmax( current_proba )\n",
        "            y_i = self.class_names[ y_i ]\n",
        "            y_pred.append( y_i )\n",
        "        y_pred = np.array( y_pred ) \n",
        "            \n",
        "        return y_pred\n",
        "    \n",
        "    def get_embeddings(self, x, batch_size=1024):\n",
        "        pass\n",
        "    \n",
        "    # no mixup version\n",
        "    \"\"\"def fit(self, x_train, y_train, x_val, y_val,\n",
        "            start_lr=0.008, end_lr=0.0001, batch_size=8192, epochs=100):\n",
        "        \n",
        "        def train(dataloader, loss_fn, optimizer):\n",
        "            self.model.train()\n",
        "            i = 0\n",
        "            start_time = datetime.now()\n",
        "            ema_loss = None\n",
        "            log_frequency = int(len(dataloader) / 10.0)\n",
        "            for x, y in dataloader:\n",
        "                x = x.to( self.device )\n",
        "                y = y.to( self.device )\n",
        "                y_pred = self.model(x)\n",
        "                loss = loss_fn(y_pred, y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                i += 1\n",
        "                if ema_loss is None:\n",
        "                    ema_loss = loss.item()\n",
        "                else:\n",
        "                    alpha = 0.2\n",
        "                    ema_loss = alpha * loss.item() + (1.0 - alpha) * ema_loss\n",
        "                if i % log_frequency == 0:\n",
        "                    total_time = datetime.now() - start_time\n",
        "                    print(f\"ema_loss: {ema_loss:>7f}  [{i * len(x):>5d}/{len(dataloader.dataset):>5d}] {total_time}\")\n",
        "        def test(dataloader, loss_fn):\n",
        "            self.model.eval()\n",
        "            \n",
        "            num_batches = len(dataloader)\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for x, y in dataloader:\n",
        "                    x = x.to( self.device )\n",
        "                    y = y.to( self.device )\n",
        "                    y_pred = self.model(x)\n",
        "                    test_loss += loss_fn(y_pred, y).item()\n",
        "            test_loss /= num_batches\n",
        "            return test_loss\n",
        "        \n",
        "        self.class_names = np.unique( y_train )\n",
        "        class_count = len( self.class_names )\n",
        "        train_dataset = CustomDataset( x_train, y_train, class_count )\n",
        "        val_dataset = CustomDataset( x_val, y_val, class_count )\n",
        "        val_data_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
        "        train_data_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        loss_function = torch.nn.CrossEntropyLoss()\n",
        "        \n",
        "        best_loss = np.inf\n",
        "        lr_step = (end_lr - start_lr) / epochs\n",
        "        current_lr = start_lr\n",
        "        for i in range(1, epochs+1):\n",
        "            \n",
        "            #optimizer = AdamW(self.model.parameters(), lr=current_lr, weight_decay=1e-2, betas=(0.9, 0.999), amsgrad=True)\n",
        "            #optimizer = torch.optim.Adam(self.model.parameters(), lr=current_lr, weight_decay=1e-5, betas=(0.9, 0.999), amsgrad=True)\n",
        "            optimizer = QHAdam(self.model.parameters(), lr=current_lr, weight_decay=1.0e-5 )\n",
        "            \n",
        "            print(\"Epoch: {} | lr: {}\".format(i, current_lr))\n",
        "            train(train_data_loader, loss_function, optimizer)\n",
        "            \n",
        "            val_loss = test(val_data_loader, loss_function)\n",
        "            print(\"Validation loss: {}\".format(val_loss))\n",
        "            if val_loss < best_loss:\n",
        "                print(\"Previous best loss: {}\".format(best_loss))\n",
        "                best_loss = val_loss\n",
        "                best_model = deepcopy( self.model )\n",
        "            \n",
        "            current_lr += lr_step\n",
        "        \n",
        "        self.model = best_model\n",
        "        self.model.eval()\n",
        "        self.model = self.model.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        \n",
        "        return self\"\"\"\n",
        "    \n",
        "    #mixup version\n",
        "    def fit(self, x_train, y_train, x_val, y_val,\n",
        "            start_lr=0.008, end_lr=0.0001, batch_size=8192, epochs=1000):\n",
        "        \n",
        "        def train(dataloader_1, dataloader_2, loss_fn, optimizer):\n",
        "            self.model.train()\n",
        "\n",
        "            i = 0\n",
        "            start_time = datetime.now()\n",
        "            ema_loss = None\n",
        "            log_frequency = int(len(dataloader_1) / 10.0)\n",
        "            for (x_1, y_1), (x_2, y_2) in zip(dataloader_1, dataloader_2):\n",
        "                lam = np.random.beta(0.2, 0.2)\n",
        "                x = lam * x_1 + (1.0 - lam) * x_2\n",
        "                y = lam * y_1 + (1.0 - lam) * y_2\n",
        "                x = x.to(self.device)\n",
        "                y = y.to(self.device)\n",
        "                y_pred = self.model(x)\n",
        "\n",
        "                loss = loss_fn(y_pred, y)\n",
        "                optimizer.zero_grad()\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "\n",
        "                i += 1\n",
        "                if ema_loss is None:\n",
        "                    ema_loss = loss.item()\n",
        "                else:\n",
        "                    alpha = 0.2\n",
        "                    ema_loss = alpha * loss.item() + (1.0 - alpha) * ema_loss\n",
        "\n",
        "                #if i % log_frequency == 0:\n",
        "                total_time = datetime.now() - start_time\n",
        "                #print(f\"ema_loss: {ema_loss:>7f}  [{i * len(x):>5d}/{len(dataloader_1.dataset):>5d}] {total_time}\")\n",
        "\n",
        "        def test(dataloader, loss_fn):\n",
        "            self.model.eval()\n",
        "            \n",
        "            num_batches = len(dataloader)\n",
        "            test_loss = 0\n",
        "            with torch.no_grad():\n",
        "                for x, y in dataloader:\n",
        "                    x = x.to( self.device )\n",
        "                    y = y.to( self.device )\n",
        "                    y_pred = self.model(x)\n",
        "                    test_loss += loss_fn(y_pred, y).item()\n",
        "            test_loss /= num_batches\n",
        "            return test_loss\n",
        "        \n",
        "        self.class_names = np.unique( y_train )\n",
        "        class_count = len( self.class_names )\n",
        "        train_dataset_1 = CustomDataset( x_train, y_train, class_count )\n",
        "        train_dataset_2 = CustomDataset( x_train, y_train, class_count )\n",
        "        train_data_loader_1 = DataLoader(train_dataset_1, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        train_data_loader_2 = DataLoader(train_dataset_2, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "        val_dataset = CustomDataset( x_val, y_val, class_count )\n",
        "        val_data_loader = DataLoader(val_dataset, batch_size=1024, shuffle=False)\n",
        "        loss_function = torch.nn.CrossEntropyLoss()\n",
        "        \n",
        "        best_loss = np.inf\n",
        "        lr_step = (end_lr - start_lr) / epochs\n",
        "        current_lr = start_lr\n",
        "        for i in range(1, epochs+1):\n",
        "            \n",
        "            #optimizer = AdamW(self.model.parameters(), lr=current_lr, weight_decay=1e-2, betas=(0.9, 0.999), amsgrad=True)\n",
        "            #optimizer = torch.optim.Adam(self.model.parameters(), lr=current_lr, weight_decay=1e-5, betas=(0.9, 0.999), amsgrad=True)\n",
        "            #if i % 20 == 0:\n",
        "            #    current_lr = 0.95 * current_lr\n",
        "            optimizer = QHAdam( self.model.parameters(), lr=current_lr, weight_decay=1.0e-5 )\n",
        "            \n",
        "            print(\"Epoch: {} | lr: {}\".format(i, current_lr))\n",
        "            train(train_data_loader_1, train_data_loader_2, loss_function, optimizer)\n",
        "            \n",
        "            val_loss = test(val_data_loader, loss_function)\n",
        "            print(\"Validation loss: {}\".format(val_loss))\n",
        "            if val_loss < best_loss:\n",
        "                print(\"Previous best loss: {}\".format(best_loss))\n",
        "                best_loss = val_loss\n",
        "                best_model = deepcopy( self.model )\n",
        "            \n",
        "            current_lr += lr_step\n",
        "        \n",
        "        self.model = best_model\n",
        "        \n",
        "        self.model = best_model\n",
        "        self.model.eval()\n",
        "        self.model = self.model.to(\"cpu\")\n",
        "        torch.cuda.empty_cache()\n",
        "        self.model = self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "        \n",
        "        return self"
      ],
      "metadata": {
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "joGNqYc4gDib",
        "outputId": "fb13ef93-737f-4b2c-fb70-f4f86b4db435"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting qhoptim\n",
            "  Downloading qhoptim-1.1.0-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: qhoptim\n",
            "Successfully installed qhoptim-1.1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### metric and cv and train function"
      ],
      "metadata": {
        "id": "Ma9GXckSgAz2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import roc_auc_score, classification_report, f1_score\n",
        "\n",
        "def gini_score(model, x, y):\n",
        "   y_pred = model.predict_proba( x )\n",
        "   if len(y_pred.shape) == 2:\n",
        "       y_pred = y_pred[:, 1]\n",
        "   gini_score = 2.0 * roc_auc_score(y, y_pred) - 1.0\n",
        "   return gini_score"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T14:37:38.866818Z",
          "iopub.execute_input": "2023-03-21T14:37:38.867253Z",
          "iopub.status.idle": "2023-03-21T14:37:38.874927Z",
          "shell.execute_reply.started": "2023-03-21T14:37:38.867219Z",
          "shell.execute_reply": "2023-03-21T14:37:38.874049Z"
        },
        "trusted": true,
        "id": "IlgccHtcgDii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import bisect\n",
        "import torch\n",
        "\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "\n",
        "from imblearn.over_sampling import SMOTE, ADASYN\n",
        "\n",
        "def DaNet_Kfold_sex(x_is_male, y_is_male):\n",
        "   i = 0\n",
        "   cv = 10\n",
        "   val_scores = []\n",
        "   k_fold = StratifiedKFold(n_splits=cv, shuffle=True, random_state=45)\n",
        "   for train_ids, val_ids in tqdm(k_fold.split(x_is_male, y_is_male), desc=\"Fitting cv classifiers\"):\n",
        "   \n",
        "       model = DANetClassifier(input_dim = len( x_is_male[ train_ids ][0] ), \n",
        "                               num_classes = len( np.unique(y_is_male) ), \n",
        "                               #layer_num=48, base_outdim=96, k=8,\n",
        "                               #layer_num=32, base_outdim=96, k=8,\n",
        "                               layer_num=32, base_outdim=64, k=5,\n",
        "                               virtual_batch_size=256, drop_rate=0.1,\n",
        "                               device=\"cuda\")\n",
        "       model.fit( x_is_male[ train_ids ], y_is_male[ train_ids ], x_is_male[ val_ids ], y_is_male[ val_ids ], start_lr=0.008, end_lr=0.0001, batch_size=2048, epochs=50 )\n",
        "       save( model, \"danet_is_male_cv_{}.pkl\".format(i) )\n",
        "       y_pred = model.predict_proba(x_is_male[ val_ids ])[:, 1]\n",
        "       save( y_pred, \"danet_pred_cv_{}.pkl\".format(i) )\n",
        "       save( y_is_male[ val_ids ], \"danet_test_cv_{}.pkl\".format(i) )\n",
        "   \n",
        "       val_score_i = gini_score(model, x_is_male[ val_ids ], y_is_male[ val_ids ])\n",
        "       val_scores.append( val_score_i )\n",
        "       print(val_score_i)\n",
        "       del model\n",
        "       gc.collect()\n",
        "       torch.cuda.empty_cache()\n",
        "       i += 1\n",
        "   print(val_scores)\n",
        "   print(\"Mean val score: {}\".format(np.mean(val_scores)))\n",
        "   return(val_scores)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T12:53:27.761490Z",
          "iopub.execute_input": "2023-03-21T12:53:27.761953Z",
          "iopub.status.idle": "2023-03-21T12:53:27.772149Z",
          "shell.execute_reply.started": "2023-03-21T12:53:27.761909Z",
          "shell.execute_reply": "2023-03-21T12:53:27.771249Z"
        },
        "trusted": true,
        "id": "Uw5zRIVGgDii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prediction and save function"
      ],
      "metadata": {
        "id": "XweJ3j4ceRvx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(name):\n",
        "  probas = []\n",
        "  for i in tqdm(range(10), desc=\"Predicting probas\"):\n",
        "      model = load(\"danet_is_male_cv_{}.pkl\".format(i))\n",
        "      probas_i = model.predict_proba(df_sex_test.drop(['user_id'], axis=1).values)[:, 1]\n",
        "      probas_i = probas_i.reshape((-1, 1))\n",
        "      probas.append(probas_i)\n",
        "      del model\n",
        "      gc.collect()\n",
        "      torch.cuda.empty_cache()\n",
        "  probas = np.hstack(probas)\n",
        "  mean_probas = np.mean( probas, axis=1 )\n",
        "\n",
        "  submission_ids = id_to_submit[\"user_id\"].values\n",
        "  submission_ids = submission_ids.reshape((-1, 1))\n",
        "  submission_predicts = mean_probas.reshape((-1, 1))\n",
        "  submission_data = np.hstack( [submission_ids, submission_predicts] )\n",
        "  my_submission_df = pd.DataFrame( data=submission_data, columns=[\"user_id\", \"is_male\"] )\n",
        "  my_submission_df[\"user_id\"] = my_submission_df[\"user_id\"].astype(int)\n",
        "  my_submission_df.to_csv(\"is_male_predicts_{}.csv\".format(name), index=False )\n",
        "  print(my_submission_df.head())"
      ],
      "metadata": {
        "id": "hVOXk9yYgDij"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training and prediction for df_danet"
      ],
      "metadata": {
        "id": "4ocRDXYgdzld"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df_danet = pd.read_csv('df_danet.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-22T04:06:20.098451Z",
          "iopub.execute_input": "2023-03-22T04:06:20.101059Z"
        },
        "trusted": true,
        "id": "TDbLiyrbgDiS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_sex, df_sex_test = make_train_data(df_danet)"
      ],
      "metadata": {
        "trusted": true,
        "id": "pSc7NybxgDiV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5641acb5-508b-4528-a4f3-6a37cfd7eeea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(144724, 564)\n",
            "1    135314\n",
            "0    128982\n",
            "Name: is_male, dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%time\n",
        "score_sex = DaNet_Kfold_sex(df_sex.drop(['user_id', 'age', 'is_male'], axis=1).values, df_sex['is_male'].values)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T12:53:27.776597Z",
          "iopub.execute_input": "2023-03-21T12:53:27.776909Z",
          "iopub.status.idle": "2023-03-21T12:53:30.196839Z",
          "shell.execute_reply.started": "2023-03-21T12:53:27.776884Z",
          "shell.execute_reply": "2023-03-21T12:53:30.195834Z"
        },
        "trusted": true,
        "id": "DI01wZupgDii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "score_sex"
      ],
      "metadata": {
        "id": "MGQ9e2YbZByR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predict('df_danet')"
      ],
      "metadata": {
        "id": "KHub-c-iZIhl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training and prediction for df_sex"
      ],
      "metadata": {
        "id": "YzdABUGSeDVt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_sex_0 = pd.read_csv('df_sex.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-22T04:06:20.098451Z",
          "iopub.execute_input": "2023-03-22T04:06:20.101059Z"
        },
        "trusted": true,
        "id": "ev0a3eoEZejQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_sex, df_sex_test = make_train_data(df_sex_0)"
      ],
      "metadata": {
        "trusted": true,
        "id": "VSjxoiGKZejS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# score_sex_0 = DaNet_Kfold_sex(df_sex.drop(['user_id', 'age', 'is_male'], axis=1).values, df_sex['is_male'].values)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T12:53:27.776597Z",
          "iopub.execute_input": "2023-03-21T12:53:27.776909Z",
          "iopub.status.idle": "2023-03-21T12:53:30.196839Z",
          "shell.execute_reply.started": "2023-03-21T12:53:27.776884Z",
          "shell.execute_reply": "2023-03-21T12:53:30.195834Z"
        },
        "trusted": true,
        "id": "ih2bIf_XZejS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# score_sex_0"
      ],
      "metadata": {
        "id": "ox-YlwVKZejU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict('df_sex')"
      ],
      "metadata": {
        "id": "1fVspBb0ZejU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# training and prediction for df_sex_1"
      ],
      "metadata": {
        "id": "rzNQ09fYeHuf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# df_sex_1 = pd.read_csv('df_sex_1.csv')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-22T04:06:20.098451Z",
          "iopub.execute_input": "2023-03-22T04:06:20.101059Z"
        },
        "trusted": true,
        "id": "GMOixHcJZfL0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# df_sex, df_sex_test = make_train_data(df_sex_1)"
      ],
      "metadata": {
        "trusted": true,
        "id": "jwwxGPWtZfL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# %%time\n",
        "# score_sex_1 = DaNet_Kfold_sex(df_sex.drop(['user_id', 'age', 'is_male'], axis=1).values, df_sex['is_male'].values)"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2023-03-21T12:53:27.776597Z",
          "iopub.execute_input": "2023-03-21T12:53:27.776909Z",
          "iopub.status.idle": "2023-03-21T12:53:30.196839Z",
          "shell.execute_reply.started": "2023-03-21T12:53:27.776884Z",
          "shell.execute_reply": "2023-03-21T12:53:30.195834Z"
        },
        "trusted": true,
        "id": "hn7VDlZ0ZfL2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# score_sex_1"
      ],
      "metadata": {
        "id": "lK-uGytyZfL3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# predict('df_sex_1')"
      ],
      "metadata": {
        "id": "sICIlZO6ZfL4"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}