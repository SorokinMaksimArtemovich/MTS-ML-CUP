{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f05e35f",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-03-18T12:12:20.098029Z",
     "iopub.status.busy": "2023-03-18T12:12:20.096530Z",
     "iopub.status.idle": "2023-03-18T12:12:20.109894Z",
     "shell.execute_reply": "2023-03-18T12:12:20.108732Z"
    },
    "papermill": {
     "duration": 0.026638,
     "end_time": "2023-03-18T12:12:20.112412",
     "exception": false,
     "start_time": "2023-03-18T12:12:20.085774",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "os.environ['OPENBLAS_NUM_THREADS'] = '1'\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2237d354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:12:20.120839Z",
     "iopub.status.busy": "2023-03-18T12:12:20.119715Z",
     "iopub.status.idle": "2023-03-18T12:12:20.456423Z",
     "shell.execute_reply": "2023-03-18T12:12:20.455349Z"
    },
    "papermill": {
     "duration": 0.343742,
     "end_time": "2023-03-18T12:12:20.459303",
     "exception": false,
     "start_time": "2023-03-18T12:12:20.115561",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gc\n",
    "import pickle\n",
    "import time\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from tqdm import tqdm\n",
    "import scipy\n",
    "import implicit\n",
    "import bisect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "97e87876",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:17:14.306556Z",
     "iopub.status.busy": "2023-03-18T12:17:14.306167Z",
     "iopub.status.idle": "2023-03-18T12:17:14.312392Z",
     "shell.execute_reply": "2023-03-18T12:17:14.311313Z"
    },
    "papermill": {
     "duration": 0.159246,
     "end_time": "2023-03-18T12:17:14.314758",
     "exception": false,
     "start_time": "2023-03-18T12:17:14.155512",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def save(obj, path, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"Saving object to {}\".format(path))\n",
    "\n",
    "    with open(path, \"wb\") as obj_file:\n",
    "        pickle.dump( obj, obj_file, protocol=pickle.HIGHEST_PROTOCOL )\n",
    "\n",
    "    if verbose:\n",
    "        print(\"Object saved to {}\".format(path))\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ebcf8dec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:12:49.972893Z",
     "iopub.status.busy": "2023-03-18T12:12:49.972496Z",
     "iopub.status.idle": "2023-03-18T12:12:49.979165Z",
     "shell.execute_reply": "2023-03-18T12:12:49.978016Z"
    },
    "papermill": {
     "duration": 0.013606,
     "end_time": "2023-03-18T12:12:49.981657",
     "exception": false,
     "start_time": "2023-03-18T12:12:49.968051",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load(path, verbose=True):\n",
    "    if verbose:\n",
    "        print(\"Loading object from {}\".format(path))\n",
    "    with open(path, \"rb\") as obj_file:\n",
    "        obj = pickle.load(obj_file)\n",
    "    if verbose:\n",
    "        print(\"Object loaded from {}\".format(path))\n",
    "    return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "661c8325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-03-18T12:12:20.467268Z",
     "iopub.status.busy": "2023-03-18T12:12:20.466836Z",
     "iopub.status.idle": "2023-03-18T12:12:49.961943Z",
     "shell.execute_reply": "2023-03-18T12:12:49.960759Z"
    },
    "papermill": {
     "duration": 29.502295,
     "end_time": "2023-03-18T12:12:49.964679",
     "exception": false,
     "start_time": "2023-03-18T12:12:20.462384",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "data = pd.read_feather('/data/dataset_full.feather', columns=[\"user_id\", \"region_name\", \"city_name\", \n",
    "                                                              \"cpe_manufacturer_name\", \"cpe_model_name\", \n",
    "                                                              \"cpe_type_cd\", \"cpe_model_os_type\",\"part_of_day\", \n",
    "                                                              \"day_name\", \"day_name_parts\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67a2020b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I use word_to_vec categorical encoder from\n",
    "# https://github.com/CameleoGrey/cameleogrey_mtsmlcup\n",
    "\n",
    "class GreyCategoricalEncoder():\n",
    "    \n",
    "    def __init__(self,\n",
    "                 shuffle_count=5, vec_size=1,\n",
    "                 window=5, n_jobs=8,\n",
    "                 min_count=1, sample=0,\n",
    "                 epochs=100, sg=0, seed=45):\n",
    "\n",
    "        self.feature_name = None\n",
    "        self.w2v_dict = None\n",
    "        self.shuffle_count = shuffle_count\n",
    "        self.vec_size = vec_size\n",
    "        self.window = window\n",
    "        self.n_jobs = n_jobs\n",
    "        self.min_count = min_count\n",
    "        self.sample = sample\n",
    "        self.epochs = epochs\n",
    "        self.sg = sg\n",
    "        self.seed = seed\n",
    "\n",
    "        pass\n",
    "\n",
    "    def fit(self, values_list, feature_name):\n",
    "        \n",
    "        self.feature_name = feature_name\n",
    "        \n",
    "        shuffled_token_rows = []\n",
    "        for i in tqdm(range(self.shuffle_count), desc=\"Generating token shuffles for fitting cat encoder\"):\n",
    "            shuffled_tokens = deepcopy(values_list)\n",
    "            np.random.shuffle(shuffled_tokens)\n",
    "            shuffled_token_rows.append( shuffled_tokens )\n",
    "        \n",
    "        w2v_dict = self.fit_word2vec_( shuffled_token_rows, size=self.vec_size,\n",
    "                                      window=self.window, n_jobs=self.n_jobs,\n",
    "                                      min_count=self.min_count, sample=self.sample,\n",
    "                                      epochs=self.epochs, sg=self.sg, seed=self.seed)\n",
    "        self.w2v_dict = w2v_dict\n",
    "\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def transform(self, values_list):\n",
    "        \n",
    "        encoded_feats_names = []\n",
    "        for i in range(self.vec_size):\n",
    "            encoded_feats_names.append( str(self.feature_name) + \"_{}\".format(i) )\n",
    "        \n",
    "        encoded_feats = self.encode_features_( values_list, verbose=True )\n",
    "        encoded_feats = np.array( encoded_feats )\n",
    "\n",
    "        return encoded_feats, encoded_feats_names\n",
    "\n",
    "    def fit_word2vec_(self, texts, size=128, window=5, n_jobs=8, min_count=1, sample=0, epochs=100, sg=0, seed=45):\n",
    "        logging.root.setLevel(level=logging.INFO)\n",
    "        w2v_model = Word2Vec(texts, vector_size=size, window=window, workers=n_jobs, min_count=min_count, sample=sample, epochs=epochs, sg=sg, seed=seed)\n",
    "        w2v_dict = dict(zip(w2v_model.wv.index_to_key, w2v_model.wv.vectors))\n",
    "        del texts\n",
    "        gc.collect()\n",
    "\n",
    "        return w2v_dict\n",
    "\n",
    "    def encode_features_(self, texts, verbose=True):\n",
    "\n",
    "        text_vectors = []\n",
    "        if verbose:\n",
    "            proc_range = tqdm(range(len(texts)), desc=\"Vectorizing texts\")\n",
    "        else:\n",
    "            proc_range = range(len(texts))\n",
    "\n",
    "        vec_size = len(self.w2v_dict[next(iter(self.w2v_dict.keys()))])\n",
    "        for i in proc_range:\n",
    "            current_vector = None\n",
    "            current_vector = self.w2v_dict[texts[i]]\n",
    "            if vec_size == 1:\n",
    "                current_vector = current_vector[0]\n",
    "\n",
    "            text_vectors.append(current_vector)\n",
    "        return text_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ab247e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_cat_encoders(df, feature_names, \n",
    "                         embedding_sizes, shuffle_counts):\n",
    "    \n",
    "    cat_encoders={}\n",
    "    for i in tqdm(range(len(feature_names)), desc=\"Fitting category encoders\"):\n",
    "        feature_list = df[feature_names[i]].to_list()\n",
    "        cat_encoders[feature_names[i]] = GreyCategoricalEncoder( shuffle_count=shuffle_counts[i], vec_size=embedding_sizes[i] )\n",
    "        cat_encoders[feature_names[i]].fit( feature_list, feature_names[i] )\n",
    "        \n",
    "    gc.collect()\n",
    "        \n",
    "    return cat_encoders\n",
    "    \n",
    "def transform_cat_features(df, cat_encoders):\n",
    "    for feature_name in tqdm(cat_encoders.keys(), desc=\"Encoding categorical features\"):\n",
    "        feature_values = df[feature_name].to_list()\n",
    "        embeddings, column_names = cat_encoders[feature_name].transform( feature_values )\n",
    "        del df[feature_name]\n",
    "        df_enc = pd.DataFrame(embeddings, columns=column_names)\n",
    "        df_enc['user_id'] = df['user_id']\n",
    "        # saving features\n",
    "        save(df_enc, '/data/utils/'+feature_name+'.pkl')\n",
    "        del(df_enc)\n",
    "        for i in tqdm(range(len(column_names)), desc=\"Replacing original \\\"{}\\\" by encoded\".format(feature_name)):\n",
    "            if len(column_names) == 1:\n",
    "                df[column_names[i]] = embeddings[:]\n",
    "            else:\n",
    "                df[column_names[i]] = embeddings[:, i]\n",
    "        \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d75adde3",
   "metadata": {},
   "outputs": [],
   "source": [
    "base_cat_feature_names = [\"user_id\", \"region_name\", \"city_name\", \n",
    "                                                              \"cpe_manufacturer_name\", \"cpe_model_name\", \n",
    "                                                              \"cpe_type_cd\", \"cpe_model_os_type\",\"part_of_day\", \n",
    "                                                              \"day_name\", \"day_name_parts\"]\n",
    "cat_encoders = fit_cat_encoders(data, \n",
    "                                 feature_names=base_cat_feature_names, \n",
    "                                 embedding_sizes=[1, 1],\n",
    "                                 #embedding_sizes=[5, 5, 5, 5, 1, 1, 1, 1, 5], \n",
    "                                 shuffle_counts=[1, 1])\n",
    "transform_cat_features(data, cat_encoders)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 327.147919,
   "end_time": "2023-03-18T12:17:37.598446",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-03-18T12:12:10.450527",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
